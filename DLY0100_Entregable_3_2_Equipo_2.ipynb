{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7283a721",
        "cell_id": "862c336df425410f8120019b3e3e8044",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Implementación de Transformers para Procesamiento de Lenguaje Natural (NLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6422d1ff",
        "cell_id": "37cac8a79a774c1f9f304a3c4e67dacb",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "\n",
        "### Objetivo\n",
        "En esta evaluación, implementaremos un modelo basado en arquitecturas de Transformers para una tarea de procesamiento de lenguaje natural (NLP), utilizando el dataset **DailyDialog**. Este conjunto de datos de diálogos permite que el modelo practique en generación de texto y comprensión de contexto en interacciones cotidianas.\n",
        "\n",
        "Usaremos TensorFlow para construir un modelo transformer básico con las siguientes características:\n",
        "- **Encoder-Decoder**: para procesar la entrada y generar salida secuencial.\n",
        "- **Atención Multi-cabezal**: para capturar dependencias a largo plazo en el diálogo.\n",
        "\n",
        "Al final, evaluaremos el modelo utilizando métricas específicas de NLP, como BLEU o ROUGE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2d9GhY4TWOrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Como funciona un transformer?"
      ],
      "metadata": {
        "id": "o5IR_61eUTNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" width=\"500\"/>\n",
        "\n",
        "1. Embeddings\n",
        "\n",
        "Transforman cada palabra o token en un vector numérico de alta dimensión. Representan relaciones semánticas entre palabras. Se combinan con codificaciones posicionales (positional encoding) para mantener el orden de la secuencia, ya que el Transformer no es secuencial como los RNN.\n",
        "\n",
        "2. Positional Encoding\n",
        "\n",
        "Como el Transformer no procesa las secuencias de forma secuencial (a diferencia de RNN o LSTM), no tiene forma de conocer el orden de las palabras. Para solucionar esto, se suman vectores llamados positional encodings a los embeddings. Estos codifican la posición de cada palabra en la secuencia usando funciones trigonométricas (seno y coseno), permitiendo al modelo captar el orden y la distancia entre tokens.\n",
        "\n",
        "3. Multi-Head Attention\n",
        "\n",
        "Es una extensión del self-attention. Ejecuta múltiples mecanismos de atención en paralelo (cabezas), cada uno capturando distintos tipos de relaciones (sintácticas, semánticas, etc.). Luego, combina sus salidas para obtener una representación más rica y completa del contexto.\n",
        "\n",
        "4. Feed-Forward Network\n",
        "\n",
        "Después de la atención, cada token pasa por una red neuronal simple (totalmente conectada) que refuerza o transforma su representación. Es la misma para todos los tokens, pero actúa de forma independiente en cada uno.\n",
        "\n",
        "5. Normalización y Residuos (Layer Norm + Skip Connections)\n",
        "\n",
        "Cada subcomponente (atención o feed-forward) está seguido por una conexión residual y una normalización por capas. Esto ayuda a estabilizar y acelerar el entrenamiento, y evita la pérdida de información relevante.\n",
        "\n",
        "6. Codificador (Encoder)\n",
        "\n",
        "Es una pila de capas que contienen los elementos anteriores. Toma la secuencia de entrada y la transforma en una representación contextualizada. Se utiliza, por ejemplo, en tareas como clasificación o traducción.\n",
        "\n",
        "7. Decodificador (Decoder)\n",
        "\n",
        "También es una pila de capas similares, pero además incluye atención cruzada (cross-attention) hacia la salida del codificador. Se usa en tareas generativas como traducción o generación de texto, donde se necesita producir una secuencia de salida palabra por palabra.\n",
        "\n",
        "8. Capa de salida\n",
        "\n",
        "Transforma la representación final del decodificador en una predicción, normalmente usando una capa lineal seguida de softmax para generar probabilidades sobre el vocabulario.\n",
        "\n"
      ],
      "metadata": {
        "id": "AbOmO6_NUif1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bca5997e",
        "cell_id": "ecc27731e16c4c9c99584d33db5b5e5c",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 1. Carga y Exploración del Dataset: DailyDialog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIjfVhTtyrY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889c0b3e-45af-4e2b-9ab3-e45ee1e963c1",
        "cell_id": "a8b92178837a4ec7b879466d8b95d265",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install datasets==2.16.0 fsspec==2023.9.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nCollecting fsspec==2023.9.2\n  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (2.0.2)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (18.1.0)\nCollecting pyarrow-hotfix (from datasets==2.16.0)\n  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.70.15)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (3.11.15)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.33.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (6.4.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.20.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (1.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.0) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.17.0)\nDownloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.14.4\n    Uninstalling datasets-2.14.4:\n      Successfully uninstalled datasets-2.14.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.9.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 fsspec-2023.9.2 pyarrow-hotfix-0.7\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WdQ3D-EiDu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99d8186-4937-4f93-9e6c-6b0a36397878",
        "cell_id": "7b7b1c435531474e94fecff95a91c15a",
        "deepnote_cell_type": "code"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=407f02448ad77aee5f49a83cd662c80283afa3fb469b0965c21d91e57cd27958\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a63cf22",
        "cell_id": "5ffbfbb99d084c058a5d5d57845717db",
        "deepnote_cell_type": "code"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ast\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9a-p0dwiBrQ",
        "cell_id": "ce1bbebcef094ea1b48dba89adb82cf9",
        "deepnote_cell_type": "code"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcWs7Nf3pRIq",
        "colab": {
          "height": 403,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "6fba6b94081243c4a1ffdce053a12091",
            "0eae20d822764eea81a2a760264a875d",
            "d24cf2af696947b1a820df79f16c4b9c",
            "385025fe491b4d6f879022b30a3eaa09",
            "285d7b359d654346a016aca4d7feffa8",
            "80376dba4d6241178d0c55dc974b1499",
            "04f36dcc0bca4c829fe0dd85f05c17c3",
            "80b6c1239fe44be59de6cc4cf7a9ded6",
            "ae24cdc3ea824fe09fda47cb2d73b91d",
            "86d4a81918b54c919e5e26a8bb5b1768",
            "5494c4ef143946d39fd4fbd4e93a7399",
            "9e2d7af578d74f1f8053b3f1dfa13146",
            "ea9e6e11cfc845878672f68e1c25d630",
            "a457eb6bfcb7435faffc938235c88e45",
            "551879fec42c4cf2bc1569a005f25bc3",
            "2a6e328b047a42b5997e528e02fe2570",
            "66177b6c265d425a92909602787f1fc0",
            "ba5947943cda48f6a6ef05df5a8e01fc",
            "caa75809a35f4b7696d51936197da1a1",
            "2873587005224c1e9d7189c9f5cc26bf",
            "b9a81897c94549faa0e8148f06d8fb76",
            "44c6c3af97774e30b50e5d327434f67e",
            "f92625824835434fa3795d69132b5354",
            "cb346b4b64644cd6b0c902e4820001a4",
            "a2a025aeae6746688f2681911865fafc",
            "88d9d9799bb74467a8f589acac589726",
            "9f859659a2674b4a991b0c1cdca39937",
            "843176fce0e1454ab0f466686d0accf2",
            "3abef6b5fe2043bf94cc5dc98df4ec4c",
            "43b6457f0b734541b55014cd519202e0",
            "6f7db747b0c64e0ea1424261cc5fc2d3",
            "da60692090eb46c980a74be049465573",
            "598e0e040e4248e5a1a5a0ed6d084c7b",
            "f958f470d0334b6294a1e9afd8d9f57d",
            "f99b6ddc6892472589ac47d13fa8f28c",
            "6b08cfd91ca74b4da1ecadf39c416b63",
            "966245339a794e9c95befa38e6b3da16",
            "7783ab41019a4e6cadde8c62c8c9e2d0",
            "8d3afc371fcf4280b3ed0907127a2a42",
            "09ba08ef60ba4241bab0454158b7327e",
            "cb566e1c81f748cdb66bd912c8cf8cf6",
            "0079147da752460193e254b67887d86a",
            "c65efcddf4c14d6e87df369d30a02a99",
            "de3f9ff09d324e0689efcb5b3612108d",
            "4c3b130dc3204ec19f945505e80cb96a",
            "a7124f55721b4e4bba8d8ac76db88d9b",
            "cf55fd399c834f38917c212579265de2",
            "af27da4f841f49eaa8762e402c4da717",
            "5beda3cfc03b4809a18bd2c1ebe5dbb2",
            "31ea60bbcfe5471082646c40e7e8671c",
            "009840ae731544358fcf6e20bb00cb39",
            "f5260a8235e74970a60622982dd7cf78",
            "d48764cb12244e4b95c9aa686505c44d",
            "ba0c286f828b4b6cada8367debf004f0",
            "cfc776f27eb64cbbaf5291e096832430",
            "a1b22609a9c94ba5bcac7ae736fb2f20",
            "26a8d2d3fa1e43cfa4a4e20dee9f1da5",
            "432005388442476b8aefdc083ef02cc5",
            "e497af94e2354eb3b838d16dbc143b7c",
            "a4cabec9ed3743cebc43b7962be87f47",
            "805ff597edf0443482150e4b5a3a3aea",
            "d38f8409dd8c4eb68595a62be866265b",
            "1fac567cb80a4418ade39d5e9fc60f8f",
            "9376082181fc4fc2af14120485ed28da",
            "d612dea1d2dd4ae58ec1dc30d582b792",
            "e417c11e105f4d0c9bd403bd9447eba7"
          ]
        },
        "outputId": "04d297e5-f153-4603-914f-4ee6caa75c7a",
        "cell_id": "64713fb3b66d48e488e7b1a46a92ac96",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Cargar el dataset DailyDialog\n",
        "dataset = load_dataset(\"daily_dialog\")\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "train_data, test_data, validation_data = dataset['train'], dataset['test'], dataset['validation']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/datasets/load.py:1429: FutureWarning: The repository for daily_dialog contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/daily_dialog\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/4.85k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fba6b94081243c4a1ffdce053a12091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/7.27k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e2d7af578d74f1f8053b3f1dfa13146"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/4.48M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f92625824835434fa3795d69132b5354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/11118 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f958f470d0334b6294a1e9afd8d9f57d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c3b130dc3204ec19f945505e80cb96a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1b22609a9c94ba5bcac7ae736fb2f20"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b75804f22ce24683af2be48c07b01a8d",
        "deepnote_cell_type": "markdown",
        "id": "pouULGWZ810q"
      },
      "source": [
        "Se cargaron las tres particiones que el dataset ya ofrece: entrenamiento (train), prueba (test) y validación (validation). Esta división es crucial para garantizar una evaluación objetiva del modelo.\n",
        "\n",
        "1. El conjunto de entrenamiento se utiliza para ajustar los pesos del modelo.\n",
        "\n",
        "2. El conjunto de validación permite monitorear el desempeño del modelo durante el entrenamiento y detectar sobreajuste.\n",
        "\n",
        "3. Finalmente, el conjunto de prueba se emplea exclusivamente para evaluar el rendimiento final del modelo sobre datos no vistos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "51cc6313bce8439a9728c9eac43d8f9b",
        "deepnote_cell_type": "markdown",
        "id": "5DD98nsW810r"
      },
      "source": [
        "## Procesamiento y preparación de la data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9b239256ddd047ae93d766e15440f052",
        "deepnote_cell_type": "markdown",
        "id": "ikRy5fOe810r"
      },
      "source": [
        "En esta etapa se transforma el dataset en un formato adecuado para tareas de modelado de lenguaje secuencial, específicamente para entrenar un modelo que aprenda a predecir la próxima oración dada una oración previa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "857f1c7356314dcc9d15412e089b9399",
        "deepnote_cell_type": "markdown",
        "id": "U4hBGha3810r"
      },
      "source": [
        "Para cada conjunto se recorre la lista de oraciones ('dialog') de cada ejemplo del dataset. Luego, se construyen pares (input → target) tomando cada oración como entrada y su oración siguiente como respuesta esperada.\n",
        "\n",
        "Para las respuestas (target), se añaden explícitamente los tokens especiales **\\<start\\>** y **\\<end\\>**.\n",
        "\n",
        "* **\\<start\\>** indica el inicio de la secuencia generada, lo que ayuda al modelo a aprender desde qué punto comenzar la predicción.\n",
        "\n",
        "* **\\<end\\>** señala el final de la respuesta esperada, lo cual es esencial para que el modelo sepa cuándo detener la generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw1ETOfgpVBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d01b5f-6471-41bc-9f12-95f70c5be4a2",
        "cell_id": "ff0658057d0f47d194914062ce6c8232",
        "deepnote_cell_type": "code"
      },
      "source": [
        "train_inputs, train_targets = [], []\n",
        "val_inputs, val_targets = [], []\n",
        "test_inputs, test_targets = [], []\n",
        "\n",
        "def procesar_dataset(dataset_split, input_list, target_list):\n",
        "    for ejemplo in dataset_split:\n",
        "        oraciones = ejemplo['dialog']\n",
        "        for i in range(len(oraciones) - 1):\n",
        "            entrada = oraciones[i].strip()\n",
        "            respuesta = \"<start> \" + oraciones[i + 1].strip() + \" <end>\"\n",
        "            input_list.append(entrada)\n",
        "            target_list.append(respuesta)\n",
        "\n",
        "# Aplicar por separado\n",
        "procesar_dataset(dataset['train'], train_inputs, train_targets)\n",
        "procesar_dataset(dataset['validation'], val_inputs, val_targets)\n",
        "procesar_dataset(dataset['test'], test_inputs, test_targets)\n",
        "\n",
        "print(\"Ejemplo entrada:\", train_inputs[9])\n",
        "print(\"Ejemplo respuesta:\", train_targets[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ejemplo entrada: Can you do push-ups ?\nEjemplo respuesta: <start> Of course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute . <end>\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9cfc5398ae7744028488f0094d75fa67",
        "deepnote_cell_type": "markdown",
        "id": "Z7pagcFQ810s"
      },
      "source": [
        "### Tokenización y vectorización del lenguaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a1d020df8c9441559e25c4f9763e3119",
        "deepnote_cell_type": "markdown",
        "id": "xsP33xi4810s"
      },
      "source": [
        "Se utiliza un tokenizador que:\n",
        "\n",
        "* No elimina signos de puntuación (filters=''), ya que estos pueden contener información útil en el contexto de los diálogos.\n",
        "\n",
        "* Convierte todo el texto a minúsculas (lower=True), lo que reduce el tamaño del vocabulario y evita que el modelo trate como diferentes palabras que solo difieren en mayúsculas.\n",
        "\n",
        "* Incluye un token especial (\\<OOV\\>) para representar aquellas palabras que no se encuentren en el vocabulario durante la inferencia, lo cual mejora la robustez del modelo frente a entradas desconocidas.\n",
        "\n",
        "Una vez entrenado el tokenizador, se convierte cada texto a una secuencia de enteros, donde cada entero representa una palabra o subpalabra. Luego, se aplica padding a las secuencias para que todas tengan la misma longitud. Esto es esencial para permitir el procesamiento por lotes en redes neuronales, ya que estas requieren entradas de forma uniforme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzFcYlqd0ZxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0995666d-b29d-4d6a-d34c-15e33bedace7",
        "cell_id": "2457f93af91847ce850533dc7193a957",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# 1. Preparar textos de entrenamiento para ajustar el tokenizer\n",
        "all_texts = train_inputs + train_targets\n",
        "\n",
        "# 2. Crear el tokenizer\n",
        "tokenizer = Tokenizer(filters='', lower=True, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# 3. Convertir textos a secuencias\n",
        "def textos_a_tensor(inputs, targets, tokenizer, max_len=None):\n",
        "    input_seq = tokenizer.texts_to_sequences(inputs)\n",
        "    target_seq = tokenizer.texts_to_sequences(targets)\n",
        "\n",
        "    # Padding\n",
        "    if not max_len:\n",
        "        max_len = max(max(len(s) for s in input_seq), max(len(s) for s in target_seq))\n",
        "\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "    target_seq = pad_sequences(target_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    return input_seq, target_seq, max_len\n",
        "\n",
        "# Aplicamos a cada split\n",
        "train_encoder_in, train_decoder_out, max_len = textos_a_tensor(train_inputs, train_targets, tokenizer)\n",
        "val_encoder_in, val_decoder_out, _ = textos_a_tensor(val_inputs, val_targets, tokenizer, max_len=max_len)\n",
        "test_encoder_in, test_decoder_out, _ = textos_a_tensor(test_inputs, test_targets, tokenizer, max_len=max_len)\n",
        "\n",
        "# Info útil para debugear esta montrosidad\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 para padding\n",
        "print(\"Tamaño del vocabulario:\", vocab_size)\n",
        "print(\"Longitud máxima de secuencias:\", max_len)\n",
        "\n",
        "print(\"<start> ID:\", tokenizer.word_index.get(\"<start>\"))\n",
        "print(\"<end> ID:\", tokenizer.word_index.get(\"<end>\"))\n",
        "\n",
        "idx = 0\n",
        "\n",
        "print(\"Texto original input:\", train_inputs[idx])\n",
        "print(\"Texto original target:\", train_targets[idx])\n",
        "print(\"Secuencia input:\", train_encoder_in[idx])\n",
        "print(\"Secuencia target:\", train_decoder_out[idx])\n",
        "print(\"Palabras recuperadas input:\", [tokenizer.index_word.get(i, '?') for i in train_encoder_in[idx] if i != 0])\n",
        "print(\"Palabras recuperadas target:\", [tokenizer.index_word.get(i, '?') for i in train_decoder_out[idx] if i != 0])\n",
        "\n",
        "print(\"Shape del input de entrenamiento:\", train_encoder_in.shape)\n",
        "print(\"Shape del output del decoder:\", train_decoder_out.shape)\n",
        "\n",
        "print(\"Secuencias vacías en input:\", np.sum(np.sum(train_encoder_in, axis=1) == 0))\n",
        "print(\"Secuencias vacías en target:\", np.sum(np.sum(train_decoder_out, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tamaño del vocabulario: 22079\nLongitud máxima de secuencias: 280\n<start> ID: 4\n<end> ID: 5\nTexto original input: Say , Jim , how about going for a few beers after dinner ?\nTexto original target: <start> You know that is tempting but is really not good for our fitness . <end>\nSecuencia input: [ 157    3 1100    3   38   39   83   20   11  207 3528  167  311    8\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\nSecuencia target: [   4    6   50   18   14 4108   34   14   64   43   52   20   78 2149\n    2    5    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\nPalabras recuperadas input: ['say', ',', 'jim', ',', 'how', 'about', 'going', 'for', 'a', 'few', 'beers', 'after', 'dinner', '?']\nPalabras recuperadas target: ['<start>', 'you', 'know', 'that', 'is', 'tempting', 'but', 'is', 'really', 'not', 'good', 'for', 'our', 'fitness', '.', '<end>']\nShape del input de entrenamiento: (76052, 280)\nShape del output del decoder: (76052, 280)\nSecuencias vacías en input: 0\nSecuencias vacías en target: 0\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f66d32a",
        "cell_id": "84f71e218ab44da1a177d30c11a170cc",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 2. Implementación del Modelo Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1722603d20064500bf17062cba35373c",
        "deepnote_cell_type": "markdown",
        "id": "LOcpx8pV810s"
      },
      "source": [
        "En esta sección se detalla la implementación del modelo Transformer, una arquitectura ampliamente utilizada en tareas de procesamiento de lenguaje natural por su capacidad para modelar relaciones contextuales complejas sin recurrencia. A continuación, se describen los componentes fundamentales del modelo, su arquitectura y los parámetros clave utilizados en su configuración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6aedc3840c1947c48d273bf546149fe0",
        "deepnote_cell_type": "markdown",
        "id": "Xm6MNtuE810s"
      },
      "source": [
        "- **Capa Embedding (Encoder y Decoder)**: Transforma los índices enteros de las palabras en vectores densos de dimensión 128. Esta representación distribuida permite que el modelo capte similitudes semánticas y relaciones contextuales. Se utiliza mask_zero=True para que los tokens de padding no afecten el entrenamiento.\n",
        "\n",
        "- **Bloque Transformer Encoder**: Compuesto por atención multi-cabeza (MultiHeadAttention), normalización por capas y una red densa feed-forward. La atención permite al modelo enfocarse en diferentes partes de la entrada simultáneamente, capturando dependencias contextuales clave dentro de la oración.\n",
        "\n",
        "- **Bloque Transformer Decoder**: Similar al encoder pero con dos mecanismos de atención: (1) atención causal (masked self-attention), que asegura que el modelo solo considere tokens previos al predecir, y (2) atención cruzada (encoder-decoder attention), que permite al decoder acceder a la representación codificada de la oración de entrada. También incluye una red feed-forward y normalización residual.\n",
        "\n",
        "- **Capa Dense (Salida)**: Proyecta la salida del decoder a un vector de probabilidades sobre el vocabulario completo mediante la activación softmax. Cada posición en la secuencia predice la siguiente palabra probable en base al contexto ya generado.\n",
        "\n",
        "- **Codificación Posicional**: Aunque se definió una función para generar codificación posicional sinusoidal, no se incluyó en la versión final del modelo debido a que reducía el rendimiento en la práctica. Esto puede estar relacionado con cómo Keras maneja internamente el enmascaramiento y la posición en secuencias con mask_zero=True.\n",
        "\n",
        "- **Optimización y pérdida**: Se utiliza el optimizador Adam por su eficiencia en tareas de NLP, junto con la pérdida sparse_categorical_crossentropy, adecuada para clasificación multiclase con etiquetas enteras. La métrica de precisión (accuracy) permite evaluar la proporción de tokens correctamente predichos durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dY8_1zyZzFm",
        "colab": {
          "height": 701,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c76490-458d-4cdf-929b-9595a653da77",
        "cell_id": "47887f861a15477fbba6fd5125d2ae97",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# --- Positional Encoding --- #\n",
        "# De momento no se esta usando, al implementarla el rendimiento real del modelo decae\n",
        "# REVISAR\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    pos = tf.cast(tf.range(seq_len)[:, tf.newaxis], dtype=tf.float32)\n",
        "    i = tf.cast(tf.range(d_model)[tf.newaxis, :], dtype=tf.float32)\n",
        "\n",
        "    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / d_model)\n",
        "    angle_rads = pos * angle_rates\n",
        "\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "\n",
        "    return pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "# --- Encoder Block ---\n",
        "def transformer_encoder_block(embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "    inputs = Input(shape=(None, embed_dim))\n",
        "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ff = Dense(ff_dim, activation='relu')(out1)\n",
        "    ff = Dense(embed_dim)(ff)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ff)\n",
        "\n",
        "    return Model(inputs, out2)\n",
        "\n",
        "# --- Decoder Block ---\n",
        "def transformer_decoder_block(embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "    decoder_inputs = Input(shape=(None, embed_dim))\n",
        "    encoder_outputs = Input(shape=(None, embed_dim))\n",
        "\n",
        "    # Masked Self-Attention con causal masking nativo de Keras\n",
        "    masked_attn = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embed_dim,\n",
        "        dropout=dropout\n",
        "    )(decoder_inputs, decoder_inputs, use_causal_mask=True)\n",
        "    masked_attn = Dropout(dropout)(masked_attn)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(decoder_inputs + masked_attn)\n",
        "\n",
        "    # Encoder-Decoder Attention\n",
        "    cross_attn = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embed_dim,\n",
        "        dropout=dropout\n",
        "    )(out1, encoder_outputs)\n",
        "    cross_attn = Dropout(dropout)(cross_attn)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + cross_attn)\n",
        "\n",
        "    # Feed Forward\n",
        "    ff = Dense(ff_dim, activation='relu')(out2)\n",
        "    ff = Dense(embed_dim)(ff)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    out3 = LayerNormalization(epsilon=1e-6)(out2 + ff)\n",
        "\n",
        "    return Model([decoder_inputs, encoder_outputs], out3)\n",
        "\n",
        "# --- Modelo completo ---\n",
        "def build_transformer(vocab_size, max_len, embed_dim=128, num_heads=4, ff_dim=512):\n",
        "    encoder_inputs = Input(shape=(max_len,))\n",
        "    decoder_inputs = Input(shape=(max_len,))\n",
        "\n",
        "    # Embeddings + Positional Encoding\n",
        "    encoder_embed = Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_inputs)\n",
        "    decoder_embed = Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_inputs)\n",
        "\n",
        "    # Encoder y Decoder\n",
        "    encoder_block = transformer_encoder_block(embed_dim, num_heads, ff_dim)\n",
        "    decoder_block = transformer_decoder_block(embed_dim, num_heads, ff_dim)\n",
        "\n",
        "    encoder_outputs = encoder_block(encoder_embed)\n",
        "    decoder_outputs = decoder_block([decoder_embed, encoder_outputs])\n",
        "\n",
        "    # Capa final\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
        "\n",
        "    return Model([encoder_inputs, decoder_inputs], outputs)\n",
        "\n",
        "# --- Compilar modelo ---\n",
        "model = build_transformer(vocab_size=vocab_size, max_len=max_len)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'functional' (of type Functional) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'functional_1' (of type Functional) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"functional_2\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,826,112\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,826,112\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m396,032\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m660,096\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m,       │  \u001b[38;5;34m2,848,191\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m22079\u001b[0m)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,826,112</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,826,112</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">660,096</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,848,191</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22079</span>)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,556,543\u001b[0m (36.46 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,556,543</span> (36.46 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,556,543\u001b[0m (36.46 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,556,543</span> (36.46 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31c5e26e",
        "cell_id": "c516fca454b74b728f34c55a00ab5ffa",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 3. Entrenamiento del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f5d71a217cc741e7a39bff0639248ff1",
        "deepnote_cell_type": "markdown",
        "id": "qe58_Hh8810t"
      },
      "source": [
        "Antes de iniciar el entrenamiento del modelo Transformer, es necesario preparar adecuadamente las entradas y salidas del decodificador. Como el modelo sigue un enfoque autoregresivo, el decodificador debe recibir como entrada la secuencia de respuesta desplazada una posición hacia la derecha (sin el último token), y como salida esperada (target) la misma secuencia desplazada una posición hacia la izquierda (sin el primer token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoRm_fk2Vv0",
        "cell_id": "0bc24fc39fcf4f5798e96c4bc8bc68b3",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def preparar_decoder_data(targets_tensor, max_len):\n",
        "    decoder_input = targets_tensor[:, :-1]\n",
        "    decoder_target = targets_tensor[:, 1:]\n",
        "\n",
        "    # Rellenar para que vuelvan a tener longitud max_len\n",
        "    decoder_input = pad_sequences(decoder_input, maxlen=max_len, padding='post')\n",
        "    decoder_target = pad_sequences(decoder_target, maxlen=max_len, padding='post')\n",
        "\n",
        "    return decoder_input, decoder_target\n",
        "\n",
        "# Usamos max_len nuevamente\n",
        "decoder_in_train, decoder_target_train = preparar_decoder_data(train_decoder_out, max_len)\n",
        "decoder_in_val, decoder_target_val = preparar_decoder_data(val_decoder_out, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b9c25031cbd7413a97dbfefefa61fc08",
        "deepnote_cell_type": "markdown",
        "id": "Smjimpza810t"
      },
      "source": [
        "A continuación, se entrena el modelo Transformer utilizando los datos preparados y se utiliza la función EarlyStopping para detener el entrenamiento automáticamente si la pérdida de validación no mejora durante tres épocas consecutivas. Esto ayuda a prevenir el sobreajuste y a conservar los pesos del modelo con el mejor desempeño en validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AKGp5QR2Zei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa34d3a-1d9c-4aa3-96a0-97664ecb3049",
        "cell_id": "5b151d9fac3d4984b2e8dadceb362f87",
        "deepnote_cell_type": "code"
      },
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',       # Métrica a vigilar\n",
        "    patience=3,               # N° de épocas sin mejora antes de detener\n",
        "    restore_best_weights=True,  # Restaurar los pesos con mejor val_loss\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [train_encoder_in, decoder_in_train],\n",
        "    decoder_target_train,\n",
        "    validation_data=([val_encoder_in, decoder_in_val], decoder_target_val),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 285ms/step - accuracy: 0.9504 - loss: 1.0336 - val_accuracy: 0.9621 - val_loss: 0.2219\nEpoch 2/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 281ms/step - accuracy: 0.9619 - loss: 0.2174 - val_accuracy: 0.9631 - val_loss: 0.2093\nEpoch 3/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 281ms/step - accuracy: 0.9634 - loss: 0.1988 - val_accuracy: 0.9637 - val_loss: 0.2030\nEpoch 4/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 272ms/step - accuracy: 0.9648 - loss: 0.1833 - val_accuracy: 0.9640 - val_loss: 0.1995\nEpoch 5/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 281ms/step - accuracy: 0.9657 - loss: 0.1721 - val_accuracy: 0.9644 - val_loss: 0.1984\nEpoch 6/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 282ms/step - accuracy: 0.9672 - loss: 0.1588 - val_accuracy: 0.9647 - val_loss: 0.1999\nEpoch 7/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 281ms/step - accuracy: 0.9688 - loss: 0.1471 - val_accuracy: 0.9649 - val_loss: 0.2013\nEpoch 8/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 281ms/step - accuracy: 0.9703 - loss: 0.1366 - val_accuracy: 0.9652 - val_loss: 0.2036\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 5.\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.src.callbacks.history.History at 0x78dca1102010>"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95335af50f1d43c2925ced5a0bc113d7",
        "deepnote_cell_type": "markdown",
        "id": "08DLvdeE810t"
      },
      "source": [
        "La función decode_sequence se usa para hacer predicciones con el modelo Transformer ya entrenado. Dado un texto de entrada, genera una respuesta palabra por palabra, comenzando con el token <start> y deteniéndose si se encuentra el token <end> o si se alcanza la cantidad máxima de palabras permitidas. Esta función permite probar cómo el modelo responde a distintos mensajes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_-u6J97KXX5",
        "cell_id": "be005639759c4858b37b159680f4fa27",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def decode_sequence(input_text, tokenizer, model, max_len=280):\n",
        "    # Tokenizar y pad la entrada\n",
        "    # print(\"Tokens entrada:\", tokenizer.texts_to_sequences([input_text])[0])\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Inicializamos decoder con solo el token <start>\n",
        "    decoder_input = [tokenizer.word_index['<start>']]\n",
        "    output = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "\n",
        "        # Pad y reshape el input del decoder\n",
        "        decoder_input_seq = pad_sequences([decoder_input], maxlen=max_len, padding='post')\n",
        "\n",
        "        # Predecimos el siguiente token\n",
        "        predictions = model.predict([input_seq, decoder_input_seq], verbose=0)\n",
        "        predicted_id = np.argmax(predictions[0, len(decoder_input)-1])\n",
        "\n",
        "        # Si predijo <end>, detenemos\n",
        "        if predicted_id == tokenizer.word_index['<end>']:\n",
        "            break\n",
        "\n",
        "        output.append(predicted_id)\n",
        "        decoder_input.append(predicted_id)\n",
        "        # print(\"Predicted ID:\", predicted_id, \"→\", tokenizer.index_word.get(predicted_id, '?'))\n",
        "\n",
        "    # Convertimos los IDs a texto\n",
        "    decoded_words = [tokenizer.index_word.get(id, '') for id in output]\n",
        "    return ' '.join(decoded_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg9YjCwIOmY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32a617c-f8a5-4a9e-bcbc-d39c9bf20133",
        "cell_id": "c18cdf8dcf164c10a3f63a3aa741ef6b",
        "deepnote_cell_type": "code"
      },
      "source": [
        "response = decode_sequence(\"what do you think\", tokenizer, model, max_len=280)\n",
        "print(\"Respuesta del modelo:\", response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Respuesta del modelo: i think i should do it .\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3ff9c13e0d93461192ad8e5601962e0e",
        "deepnote_cell_type": "markdown",
        "id": "3N_qRsTA810u"
      },
      "source": [
        "Se puede observar que el modelo genera una respuesta con una estructura gramatical correcta y una coherencia contextual aceptable. Sin embargo, esta evaluación cualitativa no es suficiente para confirmar que el modelo funciona correctamente de manera general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44608fb6",
        "cell_id": "553d56cb117e4980be571e9caabe871e",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 4. Evaluación del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c46db1bb5d8342f98464a0eee7181f3c",
        "deepnote_cell_type": "markdown",
        "id": "MuQHe2Dn810u"
      },
      "source": [
        "### Bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3c35bab3454c4e3badc7613f362cfd10",
        "deepnote_cell_type": "markdown",
        "id": "YlnNK3BF810u"
      },
      "source": [
        "En esta sección se evalua el modelo utilizando la métrica BLEU. Esta métrica mide qué tan parecida es la respuesta generada por el modelo en comparación con una respuesta esperada, analizando cuántas palabras y combinaciones de palabras coinciden. Se prueba el modelo con 100 ejemplos y se calcula un puntaje promedio para tener una idea general del rendimiento. Además, se imprimen algunas respuestas generadas para ver cómo se comporta el modelo en la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bfcfbc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7786c825-0b1e-459a-8e5c-f3d290ffc676",
        "cell_id": "19e217264fb84d8fa0914c5ed790dbe9",
        "deepnote_cell_type": "code"
      },
      "source": [
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "bleu_scores = []\n",
        "\n",
        "# Evaluamos, por ejemplo, 100 muestras del conjunto de test\n",
        "for i in range(100):\n",
        "    input_text = test_data[i]['dialog'][0]\n",
        "    reference = test_data[i]['dialog'][1]\n",
        "\n",
        "    print(\"Entrada: \", input_text)\n",
        "\n",
        "    prediction = decode_sequence(input_text, tokenizer, model, max_len=max_len)\n",
        "    print(\"Respuesta: \", prediction)\n",
        "    print(\"Referencia: \", reference)\n",
        "    print(\"\\n\")\n",
        "    ref_tokens = reference.lower().split()\n",
        "    pred_tokens = prediction.lower().split()\n",
        "\n",
        "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "print(f\"BLEU promedio en 100 muestras: {np.mean(bleu_scores):.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Entrada:  Hey man , you wanna buy some weed ? \nRespuesta:  yes , i have a nice girl .\nReferencia:   Some what ? \n\n\nEntrada:  The taxi drivers are on strike again . \nRespuesta:  then , then , please .\nReferencia:   What for ? \n\n\nEntrada:  We've managed to reduce our energy consumption in our factory by about 15 per cent in the last two years . \nRespuesta:  are you sure ?\nReferencia:   That's excellent . How have you managed that ? \n\n\nEntrada:  Believe it or not , tea is the most popular beverage in the world after water . \nRespuesta:  oh , i know what they are very smart .\nReferencia:   Well , people from Asia to Europe all enjoy tea . \n\n\nEntrada:  What are your personal weaknesses ? \nRespuesta:  i am working hard to do . i am working in my work .\nReferencia:   I ’ m afraid I ’ m a poor talker . I ’ m not comfortable talking with the people whom I have just met for the first time . That is not very good for business , so I have been studying public speaking . \n\n\nEntrada:  how long will it take us to drive to London ? \nRespuesta:  the first time is to be covered by the contract .\nReferencia:   I think it ’ s a distance of 180 kilometers from here to London , so it should be a two-hour drive on the motorway . \n\n\nEntrada:  So how did I do on my driving test ? \nRespuesta:  you should have to pay your money .\nReferencia:   Do you want the truth ? \n\n\nEntrada:  Good morning . What's the matter with you ? \nRespuesta:  i want to go to the airport .\nReferencia:   Good morning , doctor . I have a terrible headache . \n\n\nEntrada:  My dear , what's for supper ? \nRespuesta:  well , i ’ m afraid you can ’ t afford to get a bit of the room .\nReferencia:   Red cooked carp and rape with fresh mushrooms . \n\n\nEntrada:  Hello , this is Mike , Kara . \nRespuesta:  hi , mike . i am calling to hear that you .\nReferencia:   Mike ! Good to hear from you . How are you ? \n\n\nEntrada:  Sun-set hotel . May I help you ? \nRespuesta:  yes , i have a reservation for a reservation for a single room .\nReferencia:   Yes , I have booked a room for 24th . It's a double room . \n\n\nEntrada:  Are you busy tomorrow morning ? \nRespuesta:  yes , i am .\nReferencia:   I'm free . What's up ? \n\n\nEntrada:  I'm sorry I'm so late . I had a really bad day . \nRespuesta:  i know . but i don't know what you mean , i don't want to be .\nReferencia:   It's ten after six.We ' re late.But dinner is at six thirty . \n\n\nEntrada:  Nani Book Store . How can I help you ? \nRespuesta:  i want to buy some new dished i can ’ m looking for a table for a table for a table .\nReferencia:   Do you have the ' The man and the Sea ' by Ernest Hemingway ? \n\n\nEntrada:  Do you have maps of downtown area ? \nRespuesta:  yes , i do .\nReferencia:   Yes , here you are . \n\n\nEntrada:  Hi there , I want to reserve a hotel room . \nRespuesta:  what kind of room do you want ?\nReferencia:   No problem at all . Could I have your full name , please ? \n\n\nEntrada:  Could I have my bill , please ? \nRespuesta:  yes , sir . you can check your room number , please ?\nReferencia:   Certainly , sir . \n\n\nEntrada:  Good evening , madam . Can I help you ? \nRespuesta:  yes , i want to book a table for the table .\nReferencia:   Yes . I bought this sheet here yesterday . \n\n\nEntrada:  I need to buy some flowers for my wife . \nRespuesta:  what kind of pie do you want ?\nReferencia:   Perhaps you'd be interested in red roses . \n\n\nEntrada:  Room service , is there anything I can do for you ? \nRespuesta:  yes , i am looking for a room .\nReferencia:   Yes , I am afraid there are not enough outlets . \n\n\nEntrada:  Why'd you pull me over ? \nRespuesta:  yes , i did .\nReferencia:   Are you aware that you drove through a red light ? \n\n\nEntrada:  Can I have the roll of film developed here ? \nRespuesta:  yes , sir .\nReferencia:   Surely of course . \n\n\nEntrada:  Tell me a little bit about yourself , please . \nRespuesta:  yes , i am afraid you are .\nReferencia:   My name is Dunlin and I live in Beijing . I was born in 1980 . I will graduate from Peking University this July . I have majored in accounting . \n\n\nEntrada:  I really need to start eating healthier . \nRespuesta:  i have a flashlight .\nReferencia:   I have to start eating better too . \n\n\nEntrada:  Good afternoon ! Can I help you ? \nRespuesta:  yes , i am looking for a room .\nReferencia:   Could you show me where the Chinesc-style clothing is located ? I want to buy a silk coat . \n\n\nEntrada:  How are you today ? \nRespuesta:  i am going to be fine .\nReferencia:   Great , thanks . \n\n\nEntrada:  Today I want to invite you to talk about insurance you ’ re interested in . \nRespuesta:  yes , i ’ m afraid i ’ m not .\nReferencia:   Thank you . I wonder whether I can enjoy the life insurance and health insurance . \n\n\nEntrada:  Next , please . Hello , may I help you , sir ? \nRespuesta:  yes , i am calling to inform you that we have to be interested in this room .\nReferencia:   Yes , I want to send a registered airmail letter to France . \n\n\nEntrada:  we really were lucky . We got the last available table for two---and we didn't even have a reservation ! Did you see the long lines behind us ? \nRespuesta:  yes , that's right .\nReferencia:   yeah , I'm glad that we didn't have to wait long . I'm starving ! \n\n\nEntrada:  This position demands a higher writing ability , so please say something about your writing ability . \nRespuesta:  i think you should be interested in your job .\nReferencia:   Of course . I've loved writing since I was a very little boy . I won the first prize in a national composition contest when I was in middle school . After attending Nanjing University , I never give up writing . My works , such as Father's Tobacco Pipe , Open Air Cinema , The old City were respectively published China Youth Daily , Yangzi Evening News , and New Beijing . During the period of studying for my degrees of master and doctor , I paid more attention to developing my research ability and published several papers . The Impact of Internet in Chinese Political Participation , The Discipline of Remold , The Historical Direction of Chinese Administration Reform , Bribery Cases of Self governance in Chinese Villages are respectively published in Chinese Publish Administration , Beijing Due Xuebao , Theory and Society and Chinese Reform . I joined in Yangzi Evening News to work as a part-time journalist in 2006 . During this period , I've written a lot of comments , which improved my writing ability to a new level , I have full confidence in my writing ability , and I believe I can do the job well . \n\n\nEntrada:  pompous . How may I help you ? \nRespuesta:  i want to check out .\nReferencia:   Yes , I'd like to reserve a table for dinner . \n\n\nEntrada:  Ms . Wang , this is Mr . Cooper , president of Ivy Publishing . \nRespuesta:  oh , nice to meet you , mr . wang .\nReferencia:   It's nice to meet you , Mr . Cooper . \n\n\nEntrada:  Can you help me now ? \nRespuesta:  yes , i am looking for a pair of trousers .\nReferencia:   No , but I'll be able to tomorrow . \n\n\nEntrada:  Have you got any experience in advertising ? \nRespuesta:  yes , i have a major in a major .\nReferencia:   Yes , I have been working in the Public Relations Section of a company in the past two years . I plan the advertising campaign and cooperate the work of artists and typographers . Sometimes I have to do the work of a specialist when there ’ s something urgent . \n\n\nEntrada:  Hi , Dan . What's new ? \nRespuesta:  hi , mary .\nReferencia:   Where have you been ? I've been trying to get in touch with you for two days . \n\n\nEntrada:  Have you ever gotten a parking ticket ? \nRespuesta:  yes , i have to do .\nReferencia:   I've never gotten one . Have you ? \n\n\nEntrada:  How may I help you ? \nRespuesta:  i want to buy a pair of trousers .\nReferencia:   I ’ m having a problem . \n\n\nEntrada:  Good afternoon . Can I be of service ? \nRespuesta:  yes , i am calling to inform you that we have to inform you that we have to be able to do .\nReferencia:   I don't know . Can I purchase Treasury Certificates here at this counter ? \n\n\nEntrada:  Hey . Why did you take that money ? You are such a cheater ! I should send you to jail ! \nRespuesta:  i ’ m not sure .\nReferencia:   I am not cheating . When you pass go , you collect $ 200 , Everyone knows that ! \n\n\nEntrada:  Hi , is this Professor Clark ’ s office ? \nRespuesta:  yes , i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m not in this afternoon . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m\nReferencia:   This is Professor Clark speaking . \n\n\nEntrada:  Good afternoon ! Did you call for a plumber ? \nRespuesta:  i was hoping to get a little .\nReferencia:   Yes , yes I did . Please come in ! I ’ m so glad you came ! This old house is falling apart ! Come on into the bathroom . See , here , there ’ s water leaking everywhere ! \n\n\nEntrada:  Listen , Karen , I need your help . I don't know anyone here yet . \nRespuesta:  i know . i don't know what you mean .\nReferencia:   I'm glad to help you . What's wrong ? \n\n\nEntrada:  Good evening . What'll you have ? \nRespuesta:  please .\nReferencia:   I'd like a beer . What kind of beer do you have ? \n\n\nEntrada:  Hello , Mr . Black , how are you ? \nRespuesta:  hello , mr . smith .\nReferencia:   Fine , thank you , and how are you ? \n\n\nEntrada:  By the way miss , where is the toilet ? \nRespuesta:  i am in the city .\nReferencia:   Toilets are in the rear , I am afraid all the toilets are fully occupied at the moment . \n\n\nEntrada:  excuse me , could you tell me which line I ’ m supposed to stand in to buy bubble wrap and to post a package ? \nRespuesta:  you ’ re right .\nReferencia:   you can buy the bubble wrap here , but you ’ ll have to stand in line over here to post your passage . \n\n\nEntrada:  Good evening , Saliva . What's that wonderful aroma from your kitchen ? What are you doing now ? \nRespuesta:  i am just looking for a little bit .\nReferencia:   I am cooking now ! \n\n\nEntrada:  Room service.What can I do for you ? \nRespuesta:  i want to buy some new dished i want to buy a room .\nReferencia:   This is room 2012 . Where is my laundry ? You promised to send to me this morning . \n\n\nEntrada:  I'd like you to do me a favor . \nRespuesta:  what is it ?\nReferencia:   What is it ? \n\n\nEntrada:  Excuse me , may I help you ? \nRespuesta:  yes , i am calling to inform you that you are .\nReferencia:   Would you please fill some gas for me ? \n\n\nEntrada:  I'll be willing to come and talk about the financing of our imports . \nRespuesta:  ok , i'll be back to you .\nReferencia:   It can be solved by drawing a draft on us at 90 days sight . \n\n\nEntrada:  There's a new girl in school , have you seen her yet ? \nRespuesta:  yes . she is a girl .\nReferencia:   I haven't seen her yet . \n\n\nEntrada:  Take a seat inside and see what you think.So you will take the Porsche then , sir ? \nRespuesta:  i want to go to the table for the window .\nReferencia:   Yes , and I want to buy the insurance too . I think it's necessary . \n\n\nEntrada:  Gary . Could you type up this report for me ? I have to take off early this afternoon . \nRespuesta:  yes , i am . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m not sure you ’ m not sure you ’ ve got to hear that .\nReferencia:   Sure . Just leave it to me . I'll finish it . \n\n\nEntrada:  Look at the show on TV . I am watching a food show at a very famous seafood restaurant . I really want to eat at that restaurant . I am a seafood lover . \nRespuesta:  what kind of food are you going to eat ?\nReferencia:   Speaking of seafood , my mouth is watering . Let's go to the seafood restaurant in our neighborhood . \n\n\nEntrada:  Where do you want to go ? \nRespuesta:  i want to go to the airport .\nReferencia:   I'm going to the hospital . \n\n\nEntrada:  You're made a good choice . This china tea set is unusual . \nRespuesta:  i think so .\nReferencia:   Where was it from ? \n\n\nEntrada:  Hello , Miao Li , Where are you going ? \nRespuesta:  i'm going to the city centre of the city .\nReferencia:   Hello , I am going to the store to buy some fruit . \n\n\nEntrada:  I have a cell phone in my car . Now it's probably on the floor on the passenger side.Why don't you get it for me , and then I can call the police ? \nRespuesta:  alright .\nReferencia:   Alright . \n\n\nEntrada:  Good morning . Vane Theater , at your service . \nRespuesta:  i'm afraid i can't . i'm afraid i can't . i'm afraid i can't . i'm afraid i can't . i'm not . i'm afraid i can't . i'm afraid i can't . i'm afraid i can't . i'm afraid i can't . i'm not . i'm afraid i can't .\nReferencia:   Hello . I'm thinking about watching a Chinese traditional opera with a foreign girl . What's on this weekend ? \n\n\nEntrada:  All right . I want to bring everybody back on this subject . When can we start working on this ? \nRespuesta:  we can go to the office .\nReferencia:   Well , we could probably get started with a preparatory meeting this afternoon at 2:00 . \n\n\nEntrada:  What dressing would you like on the salad ? \nRespuesta:  i want to eat , please .\nReferencia:   French dressing , please . \n\n\nEntrada:  Could I have some fish ? \nRespuesta:  yes , sir . we have a small steaks and we can also have a small steaks and some water .\nReferencia:   Certainly . And what vegetables would you like ? \n\n\nEntrada:  Hi , George . I'm going to have a job interview next week . Could you give me some advice ? \nRespuesta:  yes , i am . i am going to be interested in a job .\nReferencia:   Sure . First of all , it ’ s very important for you not to be late . Job interviewers usually don ’ t think very highly of a candidate who arrives ten minutes after the appointed time , only to explain that he could not find the place or that there was heavy traffic . \n\n\nEntrada:  The boss announces the pay raise today , right ? How much do you think we'll get ? \nRespuesta:  no problem .\nReferencia:   No idea . Your guess is as good as mine . \n\n\nEntrada:  I'm sorry , our appointment has to be changed . \nRespuesta:  i know , but i can't agree .\nReferencia:   What a pity ! \n\n\nEntrada:  Good day ! Welcome to Lincoln Bank , how may we assist you ? \nRespuesta:  hello , i'm calling to inform you that we are going to be interested in a new york .\nReferencia:   Hello . I need to find out if a Receipt of Proceeds has arrived . I'm from Felix Wasserman Associates . \n\n\nEntrada:  May I help you ? \nRespuesta:  yes , i am calling to inform you that i have to cancel the room .\nReferencia:   Yes . I have to stay in your cry for just one day , can you suggest a short tour ? \n\n\nEntrada:  Hello , Sir . How can we help you today ? \nRespuesta:  hello , i am calling to inform you that i have to cancel the room .\nReferencia:   I need to find out some more information for L / C . I would like an outline of responsibilities , both ours , yours and the beneficiary , please . \n\n\nEntrada:  911 emergency . What is the problem ? \nRespuesta:  yes , i am . i am afraid i am not . i just can't find a sore throat and my husband smokes 20 or more time .\nReferencia:   I would like to report a break-in . \n\n\nEntrada:  Excuse me . How much is the chocolate bar ? \nRespuesta:  it's $ 10 .\nReferencia:   One dollar . \n\n\nEntrada:  House keeping.May I come in ? \nRespuesta:  yes , i have been here for a few weeks .\nReferencia:   Come in please . \n\n\nEntrada:  Ms . Montgomery ? This is Richard Thomas . I ’ m sorry to bother you at home , but I ’ Ve got a bit of a problem . \nRespuesta:  yes , i ’ m sorry . i ’ m afraid i ’ m not . i ’ m not sure you ’ ve got to hear that . i ’ ve got to hear that .\nReferencia:   Oh ? What ’ s wrong ? \n\n\nEntrada:  Excuse me . I have an appointment with Mr . Li at nine . May I come in ? \nRespuesta:  yes , sir .\nReferencia:   Yes , come in please . I am Mr . Li . You must be My Liu , right ? \n\n\nEntrada:  Tom , is Jenny crying ? \nRespuesta:  yes , i am .\nReferencia:   Can you take he away from me ? \n\n\nEntrada:  Hello , where can I buy an inexpensive cashmere sweater ? \nRespuesta:  yes , sir .\nReferencia:   Maybe you should look around for an outlet . \n\n\nEntrada:  Wow ! Your fruit looks really fresh ! How much are these apples ? \nRespuesta:  well , we also have a lot of cigarettes and vegetables .\nReferencia:   The apples are 30NT each . How many would you like ? \n\n\nEntrada:  Amelia , could you spare a few minutes ? \nRespuesta:  i don't know . i have to be able to go to get a look at this .\nReferencia:   sure . What do you need ? \n\n\nEntrada:  What can I show you ? \nRespuesta:  well , i am afraid i can find a student here .\nReferencia:   Do you have this shirt in a small ? \n\n\nEntrada:  the James ’ s file , Christine ? \nRespuesta:  yes , i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m afraid i ’ m not . i ’ m afraid i ’ m not\nReferencia:   I had it right here a minute ago , Mr . Emory . Umm . Just a minute ... \n\n\nEntrada:  I wonder whether I could possibly borrow your new bicycle now . \nRespuesta:  yes , i am . i just want to get a girl .\nReferencia:   Sorry , I'm using it myself.But you can use it this afternoon . \n\n\nEntrada:  Thanks for inviting me to work out with you , Joan . \nRespuesta:  i'm sorry to hear that .\nReferencia:   Don't mention it , let's go in . \n\n\nEntrada:  Excuse me . Check please . \nRespuesta:  yes , sir .\nReferencia:   OK , how was everything ? \n\n\nEntrada:  You look upset , anything wrong ? \nRespuesta:  i ’ m not sure .\nReferencia:   I'm going to quit the job . \n\n\nEntrada:  How long will you stay in New York ? \nRespuesta:  i will be fine .\nReferencia:   Just tor three days . I won't need these luggages.CouId I put them here ? \n\n\nEntrada:  Eat some more . \nRespuesta:  what kind of food do you want ?\nReferencia:   No , thanks . I'm on a diet . \n\n\nEntrada:  Excuse me . Do you know where the Admiral Hotel is ? \nRespuesta:  it's on the fourth floor .\nReferencia:   That's over on Callaway Street , isn't it ? \n\n\nEntrada:  I'm sorry I'm late . \nRespuesta:  what time do you want to go ?\nReferencia:   Better late than never . \n\n\nEntrada:  Professor Li , do you know where to buy art supplies ? \nRespuesta:  i think it ’ s in the city .\nReferencia:   For what ? Are you taking up painting ? \n\n\nEntrada:  Hello ? \nRespuesta:  hi , mary .\nReferencia:   Hi , Mr . Smith . This is Mary . Is Jenny there ? \n\n\nEntrada:  Do you speak English ? \nRespuesta:  i am . i speak english .\nReferencia:   Yes , a little . \n\n\nEntrada:  How do you get road news ? \nRespuesta:  i ’ m not sure .\nReferencia:   I've a driver mate in my car ! \n\n\nEntrada:  Can I help you ? \nRespuesta:  yes , i need to check the room .\nReferencia:   Yes , I'm in a bit of a rush . Could I have a piece of apple pie please ? \n\n\nEntrada:  I'd like to make an appointment with Dr.Cooper.Could you arrange it for me ? \nRespuesta:  yes , i will .\nReferencia:   Yes.Would tomorrow morning be all right with you ? \n\n\nEntrada:  Hey ! What are you doing here ? \nRespuesta:  hi , mary .\nReferencia:   I'm just shopping . \n\n\nEntrada:  Hello , is that Xiaogang Li who applied for the Solar Studio of our college ? \nRespuesta:  oh , that's right .\nReferencia:   Yes , speaking . \n\n\nEntrada:  I need help with the office party . \nRespuesta:  what time do you want to go ?\nReferencia:   Yes , of course . We could split it . What part would you like to do , the food or the entertainment ? \n\n\nEntrada:  Help ! Help ! \nRespuesta:  hello , this is my first .\nReferencia:   What's the matter ? \n\n\nEntrada:  What would you like to eat , sir ? \nRespuesta:  i want to eat a bottle of wine , please .\nReferencia:   Scrambled egg , bacon , three pieces of bread and a cup of tea . \n\n\nEntrada:  Where's the toilet ? \nRespuesta:  you are in the city .\nReferencia:   Over there . \n\n\nBLEU promedio en 100 muestras: 0.0463\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ffeddde126f6485a9c2531adac9cfcf0",
        "deepnote_cell_type": "markdown",
        "id": "l2De8sbz810u"
      },
      "source": [
        "El BLEU promedio obtenido en 100 muestras fue de **0.0463**, lo cual indica una baja coincidencia entre las respuestas generadas por el modelo y las respuestas de referencia del dataset. Este valor sugiere que, aunque el modelo puede producir respuestas con coherencia gramatical, no tiende a replicar o aproximarse léxicamente a las respuestas esperadas.\n",
        "\n",
        "Es importante considerar que el BLEU penaliza fuertemente la falta de coincidencia exacta de palabras o frases, por lo que este resultado no necesariamente implica que las respuestas sean incorrectas en contenido o intención, especialmente en tareas de diálogo donde puede haber múltiples respuestas válidas para una misma entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "478803e863f84bf488bc745c0280fa5b",
        "deepnote_cell_type": "markdown",
        "id": "pymSmmVy810v"
      },
      "source": [
        "### ROUGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "df20a3066ed5460683af75974bb26b8c",
        "deepnote_cell_type": "markdown",
        "id": "eyIw_4ta810v"
      },
      "source": [
        "Esta parte del código sirve para evaluar la calidad de las respuestas generadas por el modelo. Utiliza una métrica llamada ROUGE, que compara las respuestas del modelo con las respuestas reales (esperadas) y calcula qué tan parecidas son. Se evalúan tres tipos: ROUGE-1, ROUGE-2 y ROUGE-L, y al final se muestra un promedio de cada una para tener una idea general del desempeño del modelo en los datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3BDwDGKh6_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91f5401-86d1-40d5-9217-b83adcf75685",
        "cell_id": "cce174a100bf4d73baf66431358f0581",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Evaluación con ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "\n",
        "# Usa un subconjunto de test_data para acelerar el proceso si es muy grande\n",
        "for example in tqdm(test_data.select(range(100))):  # Cambia el rango si quieres evaluar más\n",
        "    input_text = example['dialog'][0]  # Asume que dialog es una lista [input, response]\n",
        "    reference_text = example['dialog'][1]  # Ground truth\n",
        "\n",
        "    generated_text = decode_sequence(input_text, tokenizer, model)\n",
        "\n",
        "    scores = scorer.score(reference_text, generated_text)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "# Promedios\n",
        "print(\"\\n\")\n",
        "print(f\"ROUGE-1 promedio: {np.mean(rouge1_scores):.4f}\")\n",
        "print(f\"ROUGE-2 promedio: {np.mean(rouge2_scores):.4f}\")\n",
        "print(f\"ROUGE-L promedio: {np.mean(rougeL_scores):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 100/100 [02:39<00:00,  1.59s/it]"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\nROUGE-1 promedio: 0.1624\nROUGE-2 promedio: 0.0464\nROUGE-L promedio: 0.1556\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b315e433ccdc41198100eae33df47b12",
        "deepnote_cell_type": "markdown",
        "id": "W9aS4o8M810v"
      },
      "source": [
        "ROUGE-1:\n",
        "Evalúa la superposición de unigramas (palabras individuales) entre la respuesta generada y la respuesta de referencia.\n",
        "\n",
        "El valor obtenido indica que existe una coincidencia moderadamente baja de palabras clave entre las respuestas del modelo y las esperadas, lo cual sugiere que el contenido generado contiene términos relevantes pero aún le falta precisión léxica.\n",
        "\n",
        "---\n",
        "ROUGE-2:\n",
        "Mide la coincidencia de bigramas (pares de palabras consecutivas) entre las respuestas generadas y las de referencia.\n",
        "\n",
        "El puntaje bajo en ROUGE-2 refleja una fluidez limitada y poca coincidencia en la construcción de frases completas. El modelo tiende a usar palabras relevantes, pero no necesariamente en el mismo orden o estructura que las respuestas humanas.\n",
        "\n",
        "---\n",
        "ROUGE-L:\n",
        "Evalúa la longitud de la subsecuencia común más larga (LCS) entre la respuesta generada y la esperada, capturando coincidencias en el orden general de los tokens.\n",
        "\n",
        "Este valor sugiere que el modelo es capaz de producir respuestas con cierta estructura coherente y secuencial, aunque con diferencias importantes respecto a la forma y al contenido exacto de las respuestas originales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9642c36",
        "cell_id": "d614d1fa23954f46aab0953040973e93",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 5. Ajuste de Hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "779c2d2907854d69aa42f982f4bc7ab9",
        "deepnote_cell_type": "markdown",
        "id": "Tc4P22Ig8104"
      },
      "source": [
        "Se creó una segunda versión del modelo (model_2) con ajustes en los hiperparámetros: se aumentó la dimensión del embedding de 128 a 256, el número de cabezas de atención de 4 a 6 y el tamaño de la capa feed-forward de 512 a 1024, con el fin de evaluar si estos cambios mejoran el rendimiento en tareas de generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uV2NGme5eSV",
        "colab": {
          "height": 701,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e85caab-e00a-4d1a-e3f8-b0aed211f357",
        "cell_id": "f8aed8fe026643b0b785acf7d7b3612c",
        "deepnote_cell_type": "code"
      },
      "source": [
        "model_2 = build_transformer(vocab_size=vocab_size, max_len=max_len, embed_dim=256, num_heads=6, ff_dim=1024)\n",
        "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'functional_3' (of type Functional) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'functional_4' (of type Functional) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"functional_5\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m5,652,224\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m5,652,224\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,104,320\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m3,682,560\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ functional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m,       │  \u001b[38;5;34m5,674,303\u001b[0m │ functional_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m22079\u001b[0m)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,652,224</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,652,224</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,104,320</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,682,560</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ functional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,674,303</span> │ functional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22079</span>)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,765,631\u001b[0m (86.84 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,765,631</span> (86.84 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,765,631\u001b[0m (86.84 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,765,631</span> (86.84 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7cae53e3f10c4dd3b29c45131485190b",
        "deepnote_cell_type": "markdown",
        "id": "1AWfHvmk8105"
      },
      "source": [
        "Entrenamos el modelo 2 con los nuevos hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZMpFIyi_D9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81787ac2-bf4d-4269-9064-0129a8521578",
        "cell_id": "b65029cff8d9484fbd171df772802cbb",
        "deepnote_cell_type": "code"
      },
      "source": [
        "hist_2 = model_2.fit(\n",
        "    [train_encoder_in, decoder_in_train],\n",
        "    decoder_target_train,\n",
        "    validation_data=([val_encoder_in, decoder_in_val], decoder_target_val),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 628ms/step - accuracy: 0.9516 - loss: 0.6731 - val_accuracy: 0.9624 - val_loss: 0.2157\nEpoch 2/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 618ms/step - accuracy: 0.9623 - loss: 0.2088 - val_accuracy: 0.9635 - val_loss: 0.2036\nEpoch 3/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 618ms/step - accuracy: 0.9639 - loss: 0.1886 - val_accuracy: 0.9641 - val_loss: 0.1985\nEpoch 4/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 617ms/step - accuracy: 0.9651 - loss: 0.1739 - val_accuracy: 0.9645 - val_loss: 0.1961\nEpoch 5/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 617ms/step - accuracy: 0.9665 - loss: 0.1608 - val_accuracy: 0.9649 - val_loss: 0.1948\nEpoch 6/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 616ms/step - accuracy: 0.9682 - loss: 0.1475 - val_accuracy: 0.9653 - val_loss: 0.1963\nEpoch 7/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 617ms/step - accuracy: 0.9698 - loss: 0.1371 - val_accuracy: 0.9654 - val_loss: 0.1990\nEpoch 8/20\n\u001b[1m1189/1189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 617ms/step - accuracy: 0.9711 - loss: 0.1281 - val_accuracy: 0.9657 - val_loss: 0.2010\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 5.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FsztcrT7yLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298e010d-464e-49d0-f5b1-987680ae21cf",
        "cell_id": "d002f6277a2e4a438e7c544bfca27eb5",
        "deepnote_cell_type": "code"
      },
      "source": [
        "response = decode_sequence(\"You are kinda cute\", tokenizer, model_2, max_len=280)\n",
        "print(\"Respuesta del modelo:\", response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Respuesta del modelo: i am glad to hear that .\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7d305e2e8074e2aabaeb060e00a7f7b",
        "deepnote_cell_type": "markdown",
        "id": "q-qtpqW18105"
      },
      "source": [
        "Se puede observar que, al igual que el modelo anterior, este genera una respuesta con una estructura gramatical correcta y una coherencia contextual aceptable. Sin embargo, esta evaluación cualitativa no es suficiente para confirmar que el modelo funciona correctamente de manera general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gfSGsLO7_eW",
        "cell_id": "402c735ca7f6446db170337491a23226",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Evaluación del Modelo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188EFASF_pwP",
        "cell_id": "3e5ed4ccccb643f090710b6b5f97bda4",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### Gráfico de perdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8b39e714def24bb9b02f541f4c383ebf",
        "deepnote_cell_type": "markdown",
        "id": "TKM_sGi88105"
      },
      "source": [
        "A continuación, se presenta el gráfico de la pérdida a lo largo de las épocas del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B0JeQ_l_edK",
        "colab": {
          "height": 561,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7698f1c-7fa0-47fd-9094-fcdce77984df",
        "cell_id": "c2a086bfc78e4fd0abf0ed1be92aba02",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Datos\n",
        "loss = hist_2.history['loss']\n",
        "epochs = range(len(loss))\n",
        "\n",
        "# Crear figura\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "# Curva de pérdida\n",
        "ax.plot(epochs, loss, label=\"Loss\", color='blue')\n",
        "\n",
        "# Marcar cada 10 épocas y el último punto\n",
        "for i in range(0, len(loss), 10):\n",
        "    ax.scatter(i, loss[i], color='blue')\n",
        "    ax.annotate(f\"{loss[i]:.4f}\",\n",
        "                xy=(i, loss[i]),\n",
        "                xytext=(i, loss[i] + 0.05),\n",
        "                ha='center',\n",
        "                fontsize=10,\n",
        "                color='blue')\n",
        "\n",
        "# Marcar el punto final (por si no es múltiplo de 10)\n",
        "last_epoch = len(loss) - 1\n",
        "if last_epoch % 10 != 0:\n",
        "    ax.scatter(last_epoch, loss[-1], color='blue')\n",
        "    ax.annotate(f\"{loss[-1]:.4f}\",\n",
        "                xy=(last_epoch, loss[-1]),\n",
        "                xytext=(last_epoch, loss[-1] + 0.05),\n",
        "                ha='center',\n",
        "                fontsize=10,\n",
        "                color='blue')\n",
        "\n",
        "# Estilo del gráfico\n",
        "ax.set_title('Pérdida', fontsize=16, fontweight=\"bold\")\n",
        "ax.set_xlabel('Época', fontsize=14)\n",
        "ax.set_ylabel('Loss', fontsize=14)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1200x500 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU4UHCpd_s7y",
        "cell_id": "1ccd7954f7104da0a033b31046c19133",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### Bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cf0da67ae5fb4e9d9ec3fc6c65041e72",
        "deepnote_cell_type": "markdown",
        "id": "7VlNmIy08106"
      },
      "source": [
        "En esta sección se evalua el modelo utilizando la métrica BLEU. Esta métrica mide qué tan parecida es la respuesta generada por el modelo en comparación con una respuesta esperada, analizando cuántas palabras y combinaciones de palabras coinciden. Se prueba el modelo con 100 ejemplos y se calcula un puntaje promedio para tener una idea general del rendimiento. Además, se imprimen algunas respuestas generadas para ver cómo se comporta el modelo en la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwgLDZx58DU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4d932e-447d-4484-ac01-45759d3a2280",
        "cell_id": "c08f114fa247487c8470d547ce04bd0e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Ejemplo de evaluación del modelo usando BLEU o ROUGE\n",
        "# predictions = model.predict(test_data)\n",
        "# print(\"BLEU Score:\", sentence_bleu(reference_sentences, predictions))\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "bleu_scores = []\n",
        "\n",
        "# Evaluamos, por ejemplo, 100 muestras del conjunto de test\n",
        "for i in range(100):\n",
        "    input_text = test_data[i]['dialog'][0]\n",
        "    reference = test_data[i]['dialog'][1]\n",
        "\n",
        "    print(\"Entrada: \", input_text)\n",
        "\n",
        "    prediction = decode_sequence(input_text, tokenizer, model_2, max_len=max_len)\n",
        "    print(\"Respuesta: \", prediction)\n",
        "    print(\"Referencia: \", reference)\n",
        "    print(\"\\n\")\n",
        "    ref_tokens = reference.lower().split()\n",
        "    pred_tokens = prediction.lower().split()\n",
        "\n",
        "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "print(f\"BLEU promedio en 100 muestras: {np.mean(bleu_scores):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Entrada:  Hey man , you wanna buy some weed ? \nRespuesta:  yes , i am .\nReferencia:   Some what ? \n\n\nEntrada:  The taxi drivers are on strike again . \nRespuesta:  that's right .\nReferencia:   What for ? \n\n\nEntrada:  We've managed to reduce our energy consumption in our factory by about 15 per cent in the last two years . \nRespuesta:  the inspection bureau is built on the inspection bureau . losses must be covered by waterproof and binding on the inspection bureau .\nReferencia:   That's excellent . How have you managed that ? \n\n\nEntrada:  Believe it or not , tea is the most popular beverage in the world after water . \nRespuesta:  i see . i like the idea .\nReferencia:   Well , people from Asia to Europe all enjoy tea . \n\n\nEntrada:  What are your personal weaknesses ? \nRespuesta:  i have a master's degree in mind . i have a master's degree in mind . i have a reading degree in writing training school .\nReferencia:   I ’ m afraid I ’ m a poor talker . I ’ m not comfortable talking with the people whom I have just met for the first time . That is not very good for business , so I have been studying public speaking . \n\n\nEntrada:  how long will it take us to drive to London ? \nRespuesta:  it will be about fifteen minutes .\nReferencia:   I think it ’ s a distance of 180 kilometers from here to London , so it should be a two-hour drive on the motorway . \n\n\nEntrada:  So how did I do on my driving test ? \nRespuesta:  you can take it .\nReferencia:   Do you want the truth ? \n\n\nEntrada:  Good morning . What's the matter with you ? \nRespuesta:  i'm going to work .\nReferencia:   Good morning , doctor . I have a terrible headache . \n\n\nEntrada:  My dear , what's for supper ? \nRespuesta:  well , i ’ m going to start with the cinema , and i ’ m going to the cinema .\nReferencia:   Red cooked carp and rape with fresh mushrooms . \n\n\nEntrada:  Hello , this is Mike , Kara . \nRespuesta:  hi , randy .\nReferencia:   Mike ! Good to hear from you . How are you ? \n\n\nEntrada:  Sun-set hotel . May I help you ? \nRespuesta:  yes , i am . i have a reservation for a single room with a bath from november 21st of august .\nReferencia:   Yes , I have booked a room for 24th . It's a double room . \n\n\nEntrada:  Are you busy tomorrow morning ? \nRespuesta:  yes , i am .\nReferencia:   I'm free . What's up ? \n\n\nEntrada:  I'm sorry I'm so late . I had a really bad day . \nRespuesta:  what ?\nReferencia:   It's ten after six.We ' re late.But dinner is at six thirty . \n\n\nEntrada:  Nani Book Store . How can I help you ? \nRespuesta:  i ’ m going to need a check .\nReferencia:   Do you have the ' The man and the Sea ' by Ernest Hemingway ? \n\n\nEntrada:  Do you have maps of downtown area ? \nRespuesta:  yes , i do .\nReferencia:   Yes , here you are . \n\n\nEntrada:  Hi there , I want to reserve a hotel room . \nRespuesta:  what time ?\nReferencia:   No problem at all . Could I have your full name , please ? \n\n\nEntrada:  Could I have my bill , please ? \nRespuesta:  sure . you can fill out this form ?\nReferencia:   Certainly , sir . \n\n\nEntrada:  Good evening , madam . Can I help you ? \nRespuesta:  yes , i'd like to buy a pair of trousers .\nReferencia:   Yes . I bought this sheet here yesterday . \n\n\nEntrada:  I need to buy some flowers for my wife . \nRespuesta:  what kind of shoes ?\nReferencia:   Perhaps you'd be interested in red roses . \n\n\nEntrada:  Room service , is there anything I can do for you ? \nRespuesta:  yes , i would like to order a room with the reservation .\nReferencia:   Yes , I am afraid there are not enough outlets . \n\n\nEntrada:  Why'd you pull me over ? \nRespuesta:  i don't know .\nReferencia:   Are you aware that you drove through a red light ? \n\n\nEntrada:  Can I have the roll of film developed here ? \nRespuesta:  yes , it is .\nReferencia:   Surely of course . \n\n\nEntrada:  Tell me a little bit about yourself , please . \nRespuesta:  i am very sorry , but i have to do .\nReferencia:   My name is Dunlin and I live in Beijing . I was born in 1980 . I will graduate from Peking University this July . I have majored in accounting . \n\n\nEntrada:  I really need to start eating healthier . \nRespuesta:  what do you want to eat ?\nReferencia:   I have to start eating better too . \n\n\nEntrada:  Good afternoon ! Can I help you ? \nRespuesta:  i ’ m going to apply for a job .\nReferencia:   Could you show me where the Chinesc-style clothing is located ? I want to buy a silk coat . \n\n\nEntrada:  How are you today ? \nRespuesta:  i'm fine .\nReferencia:   Great , thanks . \n\n\nEntrada:  Today I want to invite you to talk about insurance you ’ re interested in . \nRespuesta:  i ’ m afraid i ’ m not . i ’ m not sure i ’ m not sure what i ’ m not sure .\nReferencia:   Thank you . I wonder whether I can enjoy the life insurance and health insurance . \n\n\nEntrada:  Next , please . Hello , may I help you , sir ? \nRespuesta:  yes , i am . i have a reservation for a single room with a bath from november 21st of august .\nReferencia:   Yes , I want to send a registered airmail letter to France . \n\n\nEntrada:  we really were lucky . We got the last available table for two---and we didn't even have a reservation ! Did you see the long lines behind us ? \nRespuesta:  yes , i had a long time ago .\nReferencia:   yeah , I'm glad that we didn't have to wait long . I'm starving ! \n\n\nEntrada:  This position demands a higher writing ability , so please say something about your writing ability . \nRespuesta:  i think that , but i am not a good idea .\nReferencia:   Of course . I've loved writing since I was a very little boy . I won the first prize in a national composition contest when I was in middle school . After attending Nanjing University , I never give up writing . My works , such as Father's Tobacco Pipe , Open Air Cinema , The old City were respectively published China Youth Daily , Yangzi Evening News , and New Beijing . During the period of studying for my degrees of master and doctor , I paid more attention to developing my research ability and published several papers . The Impact of Internet in Chinese Political Participation , The Discipline of Remold , The Historical Direction of Chinese Administration Reform , Bribery Cases of Self governance in Chinese Villages are respectively published in Chinese Publish Administration , Beijing Due Xuebao , Theory and Society and Chinese Reform . I joined in Yangzi Evening News to work as a part-time journalist in 2006 . During this period , I've written a lot of comments , which improved my writing ability to a new level , I have full confidence in my writing ability , and I believe I can do the job well . \n\n\nEntrada:  pompous . How may I help you ? \nRespuesta:  i ’ m going to shanghai .\nReferencia:   Yes , I'd like to reserve a table for dinner . \n\n\nEntrada:  Ms . Wang , this is Mr . Cooper , president of Ivy Publishing . \nRespuesta:  ok , mr . smith .\nReferencia:   It's nice to meet you , Mr . Cooper . \n\n\nEntrada:  Can you help me now ? \nRespuesta:  yes , i am . i have a terrible stomach-ache and i do you ?\nReferencia:   No , but I'll be able to tomorrow . \n\n\nEntrada:  Have you got any experience in advertising ? \nRespuesta:  yes , i have . i have a computer engineer's qualification certificate .\nReferencia:   Yes , I have been working in the Public Relations Section of a company in the past two years . I plan the advertising campaign and cooperate the work of artists and typographers . Sometimes I have to do the work of a specialist when there ’ s something urgent . \n\n\nEntrada:  Hi , Dan . What's new ? \nRespuesta:  hi , john .\nReferencia:   Where have you been ? I've been trying to get in touch with you for two days . \n\n\nEntrada:  Have you ever gotten a parking ticket ? \nRespuesta:  yes , i had a long time .\nReferencia:   I've never gotten one . Have you ? \n\n\nEntrada:  How may I help you ? \nRespuesta:  i ’ m a graduate student here .\nReferencia:   I ’ m having a problem . \n\n\nEntrada:  Good afternoon . Can I be of service ? \nRespuesta:  yes , i am . i need to check the documents .\nReferencia:   I don't know . Can I purchase Treasury Certificates here at this counter ? \n\n\nEntrada:  Hey . Why did you take that money ? You are such a cheater ! I should send you to jail ! \nRespuesta:  i think so .\nReferencia:   I am not cheating . When you pass go , you collect $ 200 , Everyone knows that ! \n\n\nEntrada:  Hi , is this Professor Clark ’ s office ? \nRespuesta:  yes , i ’ m a secretary . i ’ m a job .\nReferencia:   This is Professor Clark speaking . \n\n\nEntrada:  Good afternoon ! Did you call for a plumber ? \nRespuesta:  yes , i ’ d like to .\nReferencia:   Yes , yes I did . Please come in ! I ’ m so glad you came ! This old house is falling apart ! Come on into the bathroom . See , here , there ’ s water leaking everywhere ! \n\n\nEntrada:  Listen , Karen , I need your help . I don't know anyone here yet . \nRespuesta:  i am a singer . i just can't believe you are in the world .\nReferencia:   I'm glad to help you . What's wrong ? \n\n\nEntrada:  Good evening . What'll you have ? \nRespuesta:  i want to have a haircut .\nReferencia:   I'd like a beer . What kind of beer do you have ? \n\n\nEntrada:  Hello , Mr . Black , how are you ? \nRespuesta:  i ’ m fine , mr . smith .\nReferencia:   Fine , thank you , and how are you ? \n\n\nEntrada:  By the way miss , where is the toilet ? \nRespuesta:  the front of the signs and the grand time .\nReferencia:   Toilets are in the rear , I am afraid all the toilets are fully occupied at the moment . \n\n\nEntrada:  excuse me , could you tell me which line I ’ m supposed to stand in to buy bubble wrap and to post a package ? \nRespuesta:  that ’ s right .\nReferencia:   you can buy the bubble wrap here , but you ’ ll have to stand in line over here to post your passage . \n\n\nEntrada:  Good evening , Saliva . What's that wonderful aroma from your kitchen ? What are you doing now ? \nRespuesta:  i'm just looking for a few days .\nReferencia:   I am cooking now ! \n\n\nEntrada:  Room service.What can I do for you ? \nRespuesta:  i want to buy a ticket for my room .\nReferencia:   This is room 2012 . Where is my laundry ? You promised to send to me this morning . \n\n\nEntrada:  I'd like you to do me a favor . \nRespuesta:  what ?\nReferencia:   What is it ? \n\n\nEntrada:  Excuse me , may I help you ? \nRespuesta:  yes , i ’ d like to go to the library .\nReferencia:   Would you please fill some gas for me ? \n\n\nEntrada:  I'll be willing to come and talk about the financing of our imports . \nRespuesta:  i see .\nReferencia:   It can be solved by drawing a draft on us at 90 days sight . \n\n\nEntrada:  There's a new girl in school , have you seen her yet ? \nRespuesta:  yes , i am .\nReferencia:   I haven't seen her yet . \n\n\nEntrada:  Take a seat inside and see what you think.So you will take the Porsche then , sir ? \nRespuesta:  i am not sure .\nReferencia:   Yes , and I want to buy the insurance too . I think it's necessary . \n\n\nEntrada:  Gary . Could you type up this report for me ? I have to take off early this afternoon . \nRespuesta:  yes , i am . i am very interested in the mood for the mood for you .\nReferencia:   Sure . Just leave it to me . I'll finish it . \n\n\nEntrada:  Look at the show on TV . I am watching a food show at a very famous seafood restaurant . I really want to eat at that restaurant . I am a seafood lover . \nRespuesta:  what kind of food do you like ?\nReferencia:   Speaking of seafood , my mouth is watering . Let's go to the seafood restaurant in our neighborhood . \n\n\nEntrada:  Where do you want to go ? \nRespuesta:  i want to go to the airport .\nReferencia:   I'm going to the hospital . \n\n\nEntrada:  You're made a good choice . This china tea set is unusual . \nRespuesta:  i see . i like the idea .\nReferencia:   Where was it from ? \n\n\nEntrada:  Hello , Miao Li , Where are you going ? \nRespuesta:  i am going to shanghai .\nReferencia:   Hello , I am going to the store to buy some fruit . \n\n\nEntrada:  I have a cell phone in my car . Now it's probably on the floor on the passenger side.Why don't you get it for me , and then I can call the police ? \nRespuesta:  yes , i know . i don't know what you mean .\nReferencia:   Alright . \n\n\nEntrada:  Good morning . Vane Theater , at your service . \nRespuesta:  hi , i'm calling to inform you about the manager of the manager of the interview .\nReferencia:   Hello . I'm thinking about watching a Chinese traditional opera with a foreign girl . What's on this weekend ? \n\n\nEntrada:  All right . I want to bring everybody back on this subject . When can we start working on this ? \nRespuesta:  i am not sure .\nReferencia:   Well , we could probably get started with a preparatory meeting this afternoon at 2:00 . \n\n\nEntrada:  What dressing would you like on the salad ? \nRespuesta:  i want a cup of coffee .\nReferencia:   French dressing , please . \n\n\nEntrada:  Could I have some fish ? \nRespuesta:  yes , sir . we have a special blend this morning .\nReferencia:   Certainly . And what vegetables would you like ? \n\n\nEntrada:  Hi , George . I'm going to have a job interview next week . Could you give me some advice ? \nRespuesta:  sure . what is it ?\nReferencia:   Sure . First of all , it ’ s very important for you not to be late . Job interviewers usually don ’ t think very highly of a candidate who arrives ten minutes after the appointed time , only to explain that he could not find the place or that there was heavy traffic . \n\n\nEntrada:  The boss announces the pay raise today , right ? How much do you think we'll get ? \nRespuesta:  i think so .\nReferencia:   No idea . Your guess is as good as mine . \n\n\nEntrada:  I'm sorry , our appointment has to be changed . \nRespuesta:  i see .\nReferencia:   What a pity ! \n\n\nEntrada:  Good day ! Welcome to Lincoln Bank , how may we assist you ? \nRespuesta:  i am going to apply for a job market research chemist .\nReferencia:   Hello . I need to find out if a Receipt of Proceeds has arrived . I'm from Felix Wasserman Associates . \n\n\nEntrada:  May I help you ? \nRespuesta:  i ’ m a graduate student here . i ’ m a graduate student here in china .\nReferencia:   Yes . I have to stay in your cry for just one day , can you suggest a short tour ? \n\n\nEntrada:  Hello , Sir . How can we help you today ? \nRespuesta:  i want to buy a ticket for my birthday .\nReferencia:   I need to find out some more information for L / C . I would like an outline of responsibilities , both ours , yours and the beneficiary , please . \n\n\nEntrada:  911 emergency . What is the problem ? \nRespuesta:  i am not sure . i just can't wait until payday . i have it . i have to check it .\nReferencia:   I would like to report a break-in . \n\n\nEntrada:  Excuse me . How much is the chocolate bar ? \nRespuesta:  it's $ 180 per dozen .\nReferencia:   One dollar . \n\n\nEntrada:  House keeping.May I come in ? \nRespuesta:  yes , it is .\nReferencia:   Come in please . \n\n\nEntrada:  Ms . Montgomery ? This is Richard Thomas . I ’ m sorry to bother you at home , but I ’ Ve got a bit of a problem . \nRespuesta:  i ’ m sorry , but i ’ m not sure i ’ m not going to hear that .\nReferencia:   Oh ? What ’ s wrong ? \n\n\nEntrada:  Excuse me . I have an appointment with Mr . Li at nine . May I come in ? \nRespuesta:  yes , i am .\nReferencia:   Yes , come in please . I am Mr . Li . You must be My Liu , right ? \n\n\nEntrada:  Tom , is Jenny crying ? \nRespuesta:  yes , he is .\nReferencia:   Can you take he away from me ? \n\n\nEntrada:  Hello , where can I buy an inexpensive cashmere sweater ? \nRespuesta:  what type of ground beef do you want ?\nReferencia:   Maybe you should look around for an outlet . \n\n\nEntrada:  Wow ! Your fruit looks really fresh ! How much are these apples ? \nRespuesta:  well , i think they are too .\nReferencia:   The apples are 30NT each . How many would you like ? \n\n\nEntrada:  Amelia , could you spare a few minutes ? \nRespuesta:  yes , i like the food .\nReferencia:   sure . What do you need ? \n\n\nEntrada:  What can I show you ? \nRespuesta:  i want to know what you want to do .\nReferencia:   Do you have this shirt in a small ? \n\n\nEntrada:  the James ’ s file , Christine ? \nRespuesta:  i ’ m sorry , sir . i ’ m afraid you ’ re not interested in the position .\nReferencia:   I had it right here a minute ago , Mr . Emory . Umm . Just a minute ... \n\n\nEntrada:  I wonder whether I could possibly borrow your new bicycle now . \nRespuesta:  yes , it is .\nReferencia:   Sorry , I'm using it myself.But you can use it this afternoon . \n\n\nEntrada:  Thanks for inviting me to work out with you , Joan . \nRespuesta:  you're welcome .\nReferencia:   Don't mention it , let's go in . \n\n\nEntrada:  Excuse me . Check please . \nRespuesta:  here you are .\nReferencia:   OK , how was everything ? \n\n\nEntrada:  You look upset , anything wrong ? \nRespuesta:  i have no idea .\nReferencia:   I'm going to quit the job . \n\n\nEntrada:  How long will you stay in New York ? \nRespuesta:  it ’ s about 20 minutes .\nReferencia:   Just tor three days . I won't need these luggages.CouId I put them here ? \n\n\nEntrada:  Eat some more . \nRespuesta:  i want to eat breakfast .\nReferencia:   No , thanks . I'm on a diet . \n\n\nEntrada:  Excuse me . Do you know where the Admiral Hotel is ? \nRespuesta:  yes , i have a ticket .\nReferencia:   That's over on Callaway Street , isn't it ? \n\n\nEntrada:  I'm sorry I'm late . \nRespuesta:  what ?\nReferencia:   Better late than never . \n\n\nEntrada:  Professor Li , do you know where to buy art supplies ? \nRespuesta:  i think you are right .\nReferencia:   For what ? Are you taking up painting ? \n\n\nEntrada:  Hello ? \nRespuesta:  hi , i am sorry .\nReferencia:   Hi , Mr . Smith . This is Mary . Is Jenny there ? \n\n\nEntrada:  Do you speak English ? \nRespuesta:  yes , i do .\nReferencia:   Yes , a little . \n\n\nEntrada:  How do you get road news ? \nRespuesta:  i think i can afford to rent a house .\nReferencia:   I've a driver mate in my car ! \n\n\nEntrada:  Can I help you ? \nRespuesta:  yes , i am . i need to find a new student id and i can find a book .\nReferencia:   Yes , I'm in a bit of a rush . Could I have a piece of apple pie please ? \n\n\nEntrada:  I'd like to make an appointment with Dr.Cooper.Could you arrange it for me ? \nRespuesta:  sure .\nReferencia:   Yes.Would tomorrow morning be all right with you ? \n\n\nEntrada:  Hey ! What are you doing here ? \nRespuesta:  i am just trying to find a new leaf .\nReferencia:   I'm just shopping . \n\n\nEntrada:  Hello , is that Xiaogang Li who applied for the Solar Studio of our college ? \nRespuesta:  yes , he is . he is a very nice guy .\nReferencia:   Yes , speaking . \n\n\nEntrada:  I need help with the office party . \nRespuesta:  i ’ m going to the library card .\nReferencia:   Yes , of course . We could split it . What part would you like to do , the food or the entertainment ? \n\n\nEntrada:  Help ! Help ! \nRespuesta:  i am going to the library closes in the room .\nReferencia:   What's the matter ? \n\n\nEntrada:  What would you like to eat , sir ? \nRespuesta:  i want to buy a bottle of beer .\nReferencia:   Scrambled egg , bacon , three pieces of bread and a cup of tea . \n\n\nEntrada:  Where's the toilet ? \nRespuesta:  the blue one .\nReferencia:   Over there . \n\n\nBLEU promedio en 100 muestras: 0.0268\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c0589e21c93a47d683d328000dd562d4",
        "deepnote_cell_type": "markdown",
        "id": "bLJE0j3z8106"
      },
      "source": [
        "El BLEU promedio obtenido en 100 muestras fue de 0.0263, lo que indica una baja coincidencia entre las respuestas generadas por el modelo y las respuestas de referencia del dataset. Además, se observa una reducción en el desempeño en comparación con el modelo anterior, lo que sugiere que aumentar la complejidad del modelo no necesariamente mejora su rendimiento. Este resultado plantea la necesidad de reconsiderar la arquitectura utilizada o de ajustar otros aspectos del entrenamiento para lograr un mejor desempeño."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dGPyRp7_vFQ",
        "cell_id": "1cda3fe1d3234ca0b5467b5a9616fb17",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "### Rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3576c9135b7c4ab093c90cbaeb128d48",
        "deepnote_cell_type": "markdown",
        "id": "svV8Xj2q8106"
      },
      "source": [
        "Esta parte del código sirve para evaluar la calidad de las respuestas generadas por el modelo. Utiliza una métrica llamada ROUGE, que compara las respuestas del modelo con las respuestas reales (esperadas) y calcula qué tan parecidas son. Se evalúan tres tipos: ROUGE-1, ROUGE-2 y ROUGE-L, y al final se muestra un promedio de cada una para tener una idea general del desempeño del modelo en los datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGW1Vncf-x7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1bd1ab-4390-4d9d-8c23-67476336e922",
        "cell_id": "2eda55ab6749480998cdbce5bc65c54e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Evaluación con ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "\n",
        "# Usa un subconjunto de test_data para acelerar el proceso si es muy grande\n",
        "for example in tqdm(test_data.select(range(100))):  # Cambia el rango si quieres evaluar más\n",
        "    input_text = example['dialog'][0]  # Asume que dialog es una lista [input, response]\n",
        "    reference_text = example['dialog'][1]  # Ground truth\n",
        "\n",
        "    generated_text = decode_sequence(input_text, tokenizer, model_2)\n",
        "\n",
        "    scores = scorer.score(reference_text, generated_text)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "# Promedios\n",
        "print(\"\\n\")\n",
        "print(f\"ROUGE-1 promedio: {np.mean(rouge1_scores):.4f}\")\n",
        "print(f\"ROUGE-2 promedio: {np.mean(rouge2_scores):.4f}\")\n",
        "print(f\"ROUGE-L promedio: {np.mean(rougeL_scores):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 100/100 [01:40<00:00,  1.00s/it]"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\nROUGE-1 promedio: 0.1368\nROUGE-2 promedio: 0.0201\nROUGE-L promedio: 0.1297\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1365f78154f5488cb5a5a5fe8a2a9d54",
        "deepnote_cell_type": "markdown",
        "id": "VVUqt1xP8107"
      },
      "source": [
        "En conjunto, estas métricas refuerzan la observación hecha con la puntuación BLEU: el aumento en la complejidad del modelo no solo no mejora el rendimiento, sino que en este caso lo perjudica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "385dac70",
        "cell_id": "54b5bc10a41d4e31b36936788d38d845",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 6. Presentación de Resultados y Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "49d067dd93694b8f8d10d1db1c0a7791",
        "deepnote_cell_type": "markdown",
        "id": "KUOwot2i8107"
      },
      "source": [
        "| Modelo   | Configuración Principal                 | Val_loss | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-3 |\n",
        "| -------- | --------------------------------------- | -------- | ------ | ------- | ------- | ------- |\n",
        "| modelo   | embed_dim=128, num_heads=4, ff_dim=512  | 0.1984   | 0.0463 | 0.1624  | 0.0464  | 0.1556  |\n",
        "| modelo_2 | embed_dim=256, num_heads=6, ff_dim=1024 | 0.1314   | 0.0268 | 0.1368  | 0.0201  | 0.1297  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ba9a4bf49c1d41fca6a8de4a01b12268",
        "deepnote_cell_type": "markdown",
        "id": "vUppuU5Q8107"
      },
      "source": [
        "Aunque el modelo 2 presenta una menor _val_loss_, lo que indica un mejor ajuste durante el entrenamiento, el modelo 1 demuestra un rendimiento superior en el resto de metricas. Su mayor puntaje BLEU sugiere una mejor coincidencia con las respuestas esperadas, y sus resultados más altos en ROUGE-1, ROUGE-2 y ROUGE-L reflejan una mayor coherencia local y preservación de la estructura global del texto. Por estas razones se decidió escoger el primer modelo como el mejor. No obstante, cabe destacar que el rendimiento de ambos modelos es bajo en términos absolutos, por lo que se desaconseja su implementación sin antes realizar mejoras significativas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kjSwVjRhsSD",
        "cell_id": "97d0f7040fab492c979f436e2d60ddb2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "model.save(\"NPL.keras\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jphx_OOY5bP7",
        "cell_id": "5a7db88567214e90aacc970d1acbbd76",
        "deepnote_cell_type": "code"
      },
      "source": [
        "model_2.save(\"NPL_2.keras\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "49b739c957e7401a86dba451b29a3d0e",
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  }
}